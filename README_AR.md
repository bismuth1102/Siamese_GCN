# Siamese_GCN
التعلم العميق على سيامي GCN باستخدام pytorch. <br>
تم إنشاء رسمين بيانيين لكل جزيء وتحليل التشابه بين الجزيئات. <br>
تم اختبار الأداء بناءً على نماذج وهياكل بيانات مختلفة. <br>

****

* اختبرت على مجموعة بيانات الاختبار وأظهرت الأداء (الدقة و auc) في كل مرة بعد فترة التدريب. <br>

* لقد قارنت بين ChebConv من dgl والشبكات التلافيفية للرسم البياني المشترك عند تدريب شبكة Siamese ، و ChebConv أعطى أداء أفضل. <br>

* عند تدريب Siamese_GCN ، وجدت أنه سيتقارب بشكل أسرع وأقل تقلبًا في الأداء إذا تدربت على تصنيف جزيء واحد لبضع فترات وقمت بتمرير المعلمات (الوزن والتحيز) إلى النموذج السيامي. يمكنني تمرير المعلمات لأن أوزان وتحيز هذين النموذجين لهما نفس الأشكال. <br>

* أردت اختبار ما إذا كان تمرير المعلمات بين النماذج سيجعل النماذج الأخيرة تتمتع بأداء أفضل ، لذلك صممت ثلاث مراحل من التدريب. <br>
GCN_single (تدريب على جزيء واحد) -> GCN_hinge (تدريب على السيامي) -> GCN_single. <br>
لقد استخدمت GraphConvolution من Layers.py في GCN_hinge بدلاً من ChebConv من أجل تمرير المعلمات إليها. كل مرحلة لها 40 حقبة. بعد كل مرحلة ، قمت بحفظ التنبؤات على كل جزيء في مجلد الاختبار. أخيرًا ، اختبرت أداء كل مرحلة على عشرة مجلدات اختبار. تم تقديم هذه النتائج من خلال مقارنة تنبؤات كل جزيء والحقيقة الأساسية الخاصة به. (هذه العملية في Read.py.) <br> <br> <br>

* بعد المرحلة الأولى يكون العرض: <br> <br>
الدقة: 0.8226315789473684 <br> <br>
acur: 0.9036804103185596 <br> <br> <br>

* بعد المرحلة الثانية يكون العرض: <br> <br>
الدقة: 0.6846710526315789 <br> <br>
acur: 0.6992536530470914 <br> <br> <br>

* بعد المرحلة الثانية يكون العرض: <br> <br>
الدقة: 0.7778947368421053 <br> <br>
acur: 0.8873510560941829 <br>

****

## الاستنتاجات

* استنادًا إلى [تعلم التشابه مع ارتباطات الرسم البياني ذات الترتيب العالي لتحليل شبكة الدماغ] (https://arxiv.org/abs/1811.02662) ، فإن استخدام Chebyshev متعدد الحدود في الشبكة العصبية السيامية له أداء أفضل من استخدام GCN الشائع. أثبتت ممارستي ذلك. <br>

* تمرير المعلمات عديم الفائدة إذا كان هناك نموذجان مختلفان ، حتى الاختلاف الدقيق. يحتوي GCN_single على ثلاث طبقات ، الأولى (gc1) والثانية (gc2) هي أمثلة على GraphConvolution من الطبقات py ، ويمكنني تمرير المعلمات إلى هاتين الطبقتين. الطبقة الثالثة هي طبقة خطية (nn. خطية) ، ولا يمكنني تمرير المعلمات إلى هذه الطبقة. إذا قمت ببناء نموذج GCN_single وقمت بتدريبه على 40 حقبة ، مما يعني أن هذا النموذج قد تقارب ، ثم قمت ببناء نموذج GCN_single جديد ، ومرّر المعلمات إلى gc1 و gc2 ، وترك الطبقة الخطية تتم تهيئتها بشكل عشوائي ، فإن الأداء سيء بالفعل. <br>


* بالنسبة لتمرير المعلمات بين GCN_single و GCN_hinge ، فهي غير مجدية لأن لها طبقات مختلفة. <br>

* بالنسبة لهذا المشروع ، يكون التدريب على وحدة معالجة الرسومات أبطأ من وحدة المعالجة المركزية. أعتقد أن السبب في ذلك هو أن الجزيئات لها هياكل بيانية صغيرة ، فإن وحدة معالجة الرسوميات (GPU) ليست جيدة في تحليل الرسم البياني الصغير. <br>

* GCN_hinge له طبقتان. وزن الطبقة الأولى ، سواء باستخدام ChebConv أو GCN المشترك ، له شكل الميزة \ * المخفية. وزن الطبقة الثانية له شكل مخفي \ * n. بعد اجتياز الحد الأقصى للتجميع أخيرًا ، يمكنني الحصول على متجه 1 * n. في البداية كان الأداء سيئًا دائمًا عندما اخترت n = 10. ثم وجدت أنه عندما يصبح n أصغر ، سيكون الأداء أفضل. أعتقد أن هذا أيضًا بسبب الهياكل البيانية الصغيرة للجزيئات. لا تحتاج الجزيئات إلى الكثير من الميزات لوصفها. <br>


